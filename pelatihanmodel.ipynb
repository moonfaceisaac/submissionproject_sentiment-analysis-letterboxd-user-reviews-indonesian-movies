{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "308e7c2b",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ddebdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import csv\n",
    "# import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, GRU, Dense\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a193934",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7d475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>Anjingg, KerenBercerita tentang seorang pemuda...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>3 stars for the production design and costume ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Agak Laen</td>\n",
       "      <td>panjaaangggg bet dan part komedi yg pecah buat...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>pros :- brutal &amp; expertly crafted martial arts...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>I was expecting more action and comedy, tbh.</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>Kill Bill vibes.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>Agak Laen</td>\n",
       "      <td>alamak setannya batak</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>satu dari sedikit film adaptasi novel yang ber...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>\"Aku menceintaimu iteung, tetapi aku tak bisa ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>Seperti Dendam, Rindu Harus Dibayar Tuntasfill...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Agak Laen</td>\n",
       "      <td>Please just get rid of the annoying in-your-fa...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Agak Laen</td>\n",
       "      <td>\"KAU PUN LEMAH OTAK\"</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Agak Laen</td>\n",
       "      <td>Agak Laen bapakmu</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>Tidak seharusnya saya menonton film seperti in...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Agak Laen</td>\n",
       "      <td>komedi di film ini mulus jalannya, udah gitu j...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>CINEMATOGRAPHY : 8VISUAL : 7STORY &amp; PLOT : 7DI...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>Agak Laen</td>\n",
       "      <td>2 jam aku kebuang ugghhh. gatau kenapa banyak ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>\"Aku nggak bisa ngaceng..\" - Ajo KawirTapi wal...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>From acclaimed Indonesian director Edwin (Post...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>Vengeance Is Mine, All Others Pay Cash</td>\n",
       "      <td>Vibes indonesian movie 70's</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Movie Name  \\\n",
       "3189  Vengeance Is Mine, All Others Pay Cash   \n",
       "3040  Vengeance Is Mine, All Others Pay Cash   \n",
       "476                                Agak Laen   \n",
       "3166  Vengeance Is Mine, All Others Pay Cash   \n",
       "2595  Vengeance Is Mine, All Others Pay Cash   \n",
       "2933  Vengeance Is Mine, All Others Pay Cash   \n",
       "826                                Agak Laen   \n",
       "2236  Vengeance Is Mine, All Others Pay Cash   \n",
       "2952  Vengeance Is Mine, All Others Pay Cash   \n",
       "3408  Vengeance Is Mine, All Others Pay Cash   \n",
       "903                                Agak Laen   \n",
       "293                                Agak Laen   \n",
       "905                                Agak Laen   \n",
       "2991  Vengeance Is Mine, All Others Pay Cash   \n",
       "274                                Agak Laen   \n",
       "2163  Vengeance Is Mine, All Others Pay Cash   \n",
       "852                                Agak Laen   \n",
       "2976  Vengeance Is Mine, All Others Pay Cash   \n",
       "2636  Vengeance Is Mine, All Others Pay Cash   \n",
       "2242  Vengeance Is Mine, All Others Pay Cash   \n",
       "\n",
       "                                                 Review  Rating  \n",
       "3189  Anjingg, KerenBercerita tentang seorang pemuda...     5.0  \n",
       "3040  3 stars for the production design and costume ...     4.0  \n",
       "476   panjaaangggg bet dan part komedi yg pecah buat...     3.0  \n",
       "3166  pros :- brutal & expertly crafted martial arts...     NaN  \n",
       "2595       I was expecting more action and comedy, tbh.     2.5  \n",
       "2933                                   Kill Bill vibes.     NaN  \n",
       "826                               alamak setannya batak     4.0  \n",
       "2236  satu dari sedikit film adaptasi novel yang ber...     4.0  \n",
       "2952  \"Aku menceintaimu iteung, tetapi aku tak bisa ...     5.0  \n",
       "3408  Seperti Dendam, Rindu Harus Dibayar Tuntasfill...     NaN  \n",
       "903   Please just get rid of the annoying in-your-fa...     3.0  \n",
       "293                                \"KAU PUN LEMAH OTAK\"     4.5  \n",
       "905                                   Agak Laen bapakmu     4.0  \n",
       "2991  Tidak seharusnya saya menonton film seperti in...     NaN  \n",
       "274   komedi di film ini mulus jalannya, udah gitu j...     3.0  \n",
       "2163  CINEMATOGRAPHY : 8VISUAL : 7STORY & PLOT : 7DI...     2.5  \n",
       "852   2 jam aku kebuang ugghhh. gatau kenapa banyak ...     1.0  \n",
       "2976  \"Aku nggak bisa ngaceng..\" - Ajo KawirTapi wal...     4.0  \n",
       "2636  From acclaimed Indonesian director Edwin (Post...     NaN  \n",
       "2242                        Vibes indonesian movie 70's     4.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"letterboxd_reviews_clean.csv\")\n",
    "\n",
    "\n",
    "df = df.drop(columns=[\"Username\"])\n",
    "\n",
    "\n",
    "df['Movie Name'] = df['Movie Name'].apply(lambda name: (\n",
    "    'Agak Laen' if  'Agak Laen' in name else\n",
    "    'Vengeance Is Mine, All Others Pay Cash' if 'Vengeance' in name else name\n",
    "))\n",
    "\n",
    "df.sample(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1ad0a",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b24eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3486 entries, 0 to 3599\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Movie Name  3486 non-null   object \n",
      " 1   Review      3486 non-null   object \n",
      " 2   Rating      3486 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 108.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.788583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.840370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rating\n",
       "count  3486.000000\n",
       "mean      3.788583\n",
       "std       0.840370\n",
       "min       0.500000\n",
       "25%       3.500000\n",
       "50%       4.000000\n",
       "75%       4.500000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.duplicated().sum()\n",
    "df.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['Review'].str.strip().astype(bool)]\n",
    "\n",
    "df_cleaned.info()\n",
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714efc7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "- Konversi Lowercase\n",
    "- Penghilangan karakter (i.e $,%,^,etc)\n",
    "- Penghilangan Emoji\n",
    "- Lemmatizer\n",
    "- Stemmer\n",
    "- Stopwords\n",
    "- Tokenisasi\n",
    "- Kembali ke bentuk kalimat\n",
    "\n",
    "Semua dilakukan untuk kedua bahasa Inggris dan Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3721aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                        u\"\\U0001F600-\\U0001F64F\"  \n",
    "                        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                        u\"\\U00002702-\\U000027B0\"\n",
    "                        u\"\\U000024C2-\\U0001F251\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "factory = StemmerFactory()\n",
    "indo_stemmer = factory.create_stemmer()\n",
    "\n",
    "try:\n",
    "    stop_words_id = set(stopwords.words('indonesian'))  \n",
    "except:\n",
    "    stop_words_id = set()  \n",
    "\n",
    "\n",
    "all_stopwords = stop_words_en.union(stop_words_id)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_emoji(text)\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in all_stopwords:\n",
    "            continue\n",
    "        try:\n",
    "            \n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            processed_tokens.append(lemma)\n",
    "        except:\n",
    "            \n",
    "            stem = indo_stemmer.stem(word)\n",
    "            processed_tokens.append(stem)\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "\n",
    "df_cleaned['Processed Review'] = df_cleaned['Review'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74291ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Movie Name                                             Review  Rating  \\\n",
      "0   Agak Laen  Nggak ada yang ngasih tau kalau ini film prakt...     3.5   \n",
      "1   Agak Laen  THIS IS SO FRESHHHHHH FROM COMEDY HORROR INDON...     5.0   \n",
      "2   Agak Laen  Menurut saya film ini sangat amat lucu, setiap...     5.0   \n",
      "4   Agak Laen  dari awal release udah rewatch sampe 100 kali ...     3.5   \n",
      "5   Agak Laen                                      Agak Laen bah     5.0   \n",
      "6   Agak Laen  It was a fun and fresh thing to bring haunted ...     4.0   \n",
      "7   Agak Laen                             ketawa habis habisannn     4.0   \n",
      "8   Agak Laen  mmm lucu sih tp ga selucu itu jg soalnya jokes...     2.0   \n",
      "9   Agak Laen  Alur FRESHHH dan gua rasa ini REAL CRIMEEE BRO...     3.0   \n",
      "10  Agak Laen                          surprisingly entertaining     4.0   \n",
      "\n",
      "                                     Processed Review  \n",
      "0             nggak ngasih tau film praktik pesugihan  \n",
      "1   freshhhhhh comedy horror indonesia udh ga nont...  \n",
      "2   film lucu komedinya memiliki keunikan tersendi...  \n",
      "4    release udah rewatch sampe 100 kali keknya lebay  \n",
      "5                                            laen bah  \n",
      "6   fun fresh thing bring haunted house movie joke...  \n",
      "7                              ketawa habis habisannn  \n",
      "8   mmm lucu sih tp ga selucu jg jokesnya bbrp ga ...  \n",
      "9   alur freshhh gua real crimeee broooo gmn lu mi...  \n",
      "10                          surprisingly entertaining  \n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.head(10))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8617e43",
   "metadata": {},
   "source": [
    "# Data Labelisasi\n",
    "- Pada tahap ini, saya menerapkan deteksi bahasa, karena keseluruhan ulasan menggunakan bahasa Inggris dan bahasa Indonesia\n",
    "- Bahasa Inggris menggunakan TextBlob untuk mendeteksi Sentimen\n",
    "- Bahasa Indonesia menggunakana indobert-base-p1 untuk mendeteksi Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2c77239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review Language Sentiment\n",
      "0     Nggak ada yang ngasih tau kalau ini film prakt...       id  positive\n",
      "1     THIS IS SO FRESHHHHHH FROM COMEDY HORROR INDON...       id  positive\n",
      "2     Menurut saya film ini sangat amat lucu, setiap...       id  positive\n",
      "4     dari awal release udah rewatch sampe 100 kali ...       id  positive\n",
      "5                                         Agak Laen bah       id  positive\n",
      "...                                                 ...      ...       ...\n",
      "3594  Ratu Felisha is so horrifying in a good way.PS...       en  positive\n",
      "3595  Asik, gendeng, dan liar betul nih perjalanan a...       id  positive\n",
      "3596  Baru ini nonton film Indonesia yang banyak ken...       id  positive\n",
      "3598  Sebenernya isu2 yg bersembunyi di film ini tuh...       id  positive\n",
      "3599  Film yang cukup liar,brutal dan nyentrik mungk...       id  positive\n",
      "\n",
      "[3486 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "indonesian_sentiment_model = pipeline(\"sentiment-analysis\", model=\"indobenchmark/indobert-base-p1\", framework=\"pt\")\n",
    "\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        if not text or not text.strip():\n",
    "            return 'id' \n",
    "        lang = detect(text)\n",
    "        return lang if lang in ['id', 'en'] else 'id'\n",
    "    except LangDetectException:\n",
    "        return 'id'\n",
    "    \n",
    "def get_sentiment(text, lang):\n",
    "    if lang == \"en\":\n",
    "\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        if polarity > 0:\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\"\n",
    "    elif lang == \"id\":\n",
    "\n",
    "        result = indonesian_sentiment_model(text)\n",
    "        if result[0]['label'] == 'LABEL_1':\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\"\n",
    "    else:\n",
    "\n",
    "        return \"neutral\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def apply_sentiment(df):\n",
    "\n",
    "    df['Language'] = df['Review'].apply(detect_language)\n",
    "    \n",
    "\n",
    "    df['Sentiment'] = df.apply(lambda row: get_sentiment(row['Review'], row['Language']), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_sentiment = apply_sentiment(df_cleaned)\n",
    "\n",
    "\n",
    "print(df_sentiment[['Review', 'Language', 'Sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d1c0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment.to_csv('df_sentiment.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea899e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Processed Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">en</th>\n",
       "      <th>negative</th>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>868</td>\n",
       "      <td>868</td>\n",
       "      <td>868</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">id</th>\n",
       "      <th>negative</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2108</td>\n",
       "      <td>2108</td>\n",
       "      <td>2108</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Movie Name  Review  Rating  Processed Review\n",
       "Language Sentiment                                              \n",
       "en       negative          433     433     433               433\n",
       "         positive          868     868     868               868\n",
       "id       negative           77      77      77                77\n",
       "         positive         2108    2108    2108              2108"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment.groupby(['Language', 'Sentiment']).agg('count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2797c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_sentiment['Processed Review']\n",
    "y = df_sentiment['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f52ac",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "- Menggunakan TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb1245d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "722ca0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>10 minute</th>\n",
       "      <th>1010</th>\n",
       "      <th>114</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>16 mm</th>\n",
       "      <th>16mm</th>\n",
       "      <th>16mm film</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>yg gw</th>\n",
       "      <th>yg kek</th>\n",
       "      <th>yg lucu</th>\n",
       "      <th>yg ngaceng</th>\n",
       "      <th>yg nonton</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yuni</th>\n",
       "      <th>zaman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3481</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3486 rows × 3710 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10  10 minute  1010  114   15   16  16 mm  16mm  16mm film   18  ...  \\\n",
       "0     0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "1     0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "2     0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "3     0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "4     0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "...   ...        ...   ...  ...  ...  ...    ...   ...        ...  ...  ...   \n",
       "3481  0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "3482  0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "3483  0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "3484  0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "3485  0.0        0.0   0.0  0.0  0.0  0.0    0.0   0.0        0.0  0.0  ...   \n",
       "\n",
       "      yg gw  yg kek  yg lucu  yg ngaceng  yg nonton  you  young  youtube  \\\n",
       "0       0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "1       0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "2       0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "3       0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "4       0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "...     ...     ...      ...         ...        ...  ...    ...      ...   \n",
       "3481    0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "3482    0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "3483    0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "3484    0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "3485    0.0     0.0      0.0         0.0        0.0  0.0    0.0      0.0   \n",
       "\n",
       "      yuni  zaman  \n",
       "0      0.0    0.0  \n",
       "1      0.0    0.0  \n",
       "2      0.0    0.0  \n",
       "3      0.0    0.0  \n",
       "4      0.0    0.0  \n",
       "...    ...    ...  \n",
       "3481   0.0    0.0  \n",
       "3482   0.0    0.0  \n",
       "3483   0.0    0.0  \n",
       "3484   0.0    0.0  \n",
       "3485   0.0    0.0  \n",
       "\n",
       "[3486 rows x 3710 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ee6f5",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1fcd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710322f",
   "metadata": {},
   "source": [
    "# Data Modelling\n",
    "Secara berturut-turut berikut 6 cara kombinasi percobaan modelling:\n",
    "\n",
    "\n",
    "    Pelatihan: Logistic Regression,    Ekstraksi Fitur: TF-IDF,    Pembagian Data: 80/20\n",
    "    Pelatihan: SVM,    Ekstraksi Fitur: TF-IDF,    Pembagian Data: 80/20\n",
    "    Pelatihan: RF,    Ekstraksi Fitur: TF-IDF,    Pembagian Data: 80/20\n",
    "    Pelatihan: RF,    Ekstraksi Fitur: TF-IDF,    Pembagian Data: 70/30\n",
    "    Pelatihan: Decision Tree,    Ekstraksi Fitur: TF-IDF,    Pembagian Data: 80/20\n",
    "    Pelatihan: Deep Learning Dense Layer with ReLu Activation, Ekstraksi Fitur: TF-IDF, Pembagian Data: 80/20      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5f2ea3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy_train: 0.8597560975609756\n",
      "Logistic Regression - accuracy_test: 0.8510028653295129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_lr = logreg.predict(X_train)\n",
    "y_pred_test_lr = logreg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_train_lr = accuracy_score(y_train, y_pred_train_lr)\n",
    "accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "\n",
    "# Print results\n",
    "print('Logistic Regression - accuracy_train:', accuracy_train_lr)\n",
    "print('Logistic Regression - accuracy_test:', accuracy_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea9979ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - accuracy_train: 0.9092539454806313\n",
      "SVM - accuracy_test: 0.8638968481375359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the SVM model (with a linear kernel for simplicity)\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Fit the model\n",
    "# svm.fit(X_train, y_train)\n",
    "svm.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_svm = svm.predict(X_train.toarray())\n",
    "y_pred_test_svm = svm.predict(X_test.toarray())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_train_svm = accuracy_score(y_train, y_pred_train_svm)\n",
    "accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "\n",
    "# Print results\n",
    "print('SVM - accuracy_train:', accuracy_train_svm)\n",
    "print('SVM - accuracy_test:', accuracy_test_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "84651876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.9913916786226685\n",
      "Random Forest - accuracy_test: 0.8553008595988538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "\n",
    "random_forest.fit(X_train.toarray(), y_train)\n",
    "\n",
    "\n",
    "y_pred_train_rf = random_forest.predict(X_train.toarray())\n",
    "y_pred_test_rf = random_forest.predict(X_test.toarray())\n",
    "\n",
    "\n",
    "accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)\n",
    "accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)\n",
    "\n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train_rf)\n",
    "print('Random Forest - accuracy_test:', accuracy_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29d63ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.990983606557377\n",
      "Random Forest - accuracy_test: 0.858508604206501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=63)\n",
    "\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "\n",
    "random_forest.fit(X2_train.toarray(), y2_train)\n",
    "\n",
    "y2_pred_train_rf = random_forest.predict(X2_train.toarray())\n",
    "y2_pred_test_rf = random_forest.predict(X2_test.toarray())\n",
    "\n",
    "\n",
    "accuracy_train_rf = accuracy_score(y2_pred_train_rf, y2_train)\n",
    "accuracy_test_rf = accuracy_score(y2_pred_test_rf, y2_test)\n",
    "\n",
    "\n",
    "print('Random Forest - accuracy_train:', accuracy_train_rf)\n",
    "print('Random Forest - accuracy_test:', accuracy_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cb488b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - accuracy_train: 0.9913916786226685\n",
      "Decision Tree - accuracy_test: 0.8237822349570201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "decision_tree.fit(X_train.toarray(), y_train)\n",
    "\n",
    "\n",
    "y_pred_train_dt = decision_tree.predict(X_train.toarray())\n",
    "y_pred_test_dt = decision_tree.predict(X_test.toarray())\n",
    "\n",
    "\n",
    "accuracy_train_dt = accuracy_score(y_pred_train_dt, y_train)\n",
    "accuracy_test_dt = accuracy_score(y_pred_test_dt, y_test)\n",
    "\n",
    "\n",
    "print('Decision Tree - accuracy_train:', accuracy_train_dt)\n",
    "print('Decision Tree - accuracy_test:', accuracy_test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c07ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 5s 19ms/step - loss: 1.0389 - accuracy: 0.5907 - val_loss: 0.7924 - val_accuracy: 0.8309\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.7296 - accuracy: 0.7981 - val_loss: 0.7234 - val_accuracy: 0.8309\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6509 - accuracy: 0.8494 - val_loss: 0.7122 - val_accuracy: 0.8309\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.5935 - accuracy: 0.8809 - val_loss: 0.7176 - val_accuracy: 0.8309\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 1s 16ms/step - loss: 0.5278 - accuracy: 0.9049 - val_loss: 0.7398 - val_accuracy: 0.8324\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 1s 16ms/step - loss: 0.4816 - accuracy: 0.9232 - val_loss: 0.7539 - val_accuracy: 0.8281\n",
      "Train Accuracy: 0.8594\n",
      "Test Accuracy : 0.8309\n"
     ]
    }
   ],
   "source": [
    "X2 = X_tfidf.toarray()\n",
    "y2 = df_sentiment['Sentiment']\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y2_int = le.fit_transform(y2)\n",
    "y2_ohe = to_categorical(y2_int, num_classes=2)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X2_scaled = scaler.fit_transform(X2)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
    "    X2_scaled, y2_ohe, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X2_train.shape[1],), activation='relu',\n",
    "    kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(34, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(15, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X2_train, y2_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X2_test, y2_test),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss_train, acc_train = model.evaluate(X2_train, y2_train, verbose=0)\n",
    "loss_test, acc_test = model.evaluate(X2_test, y2_test, verbose=0)\n",
    "\n",
    "print(f\"Train Accuracy: {acc_train:.4f}\")\n",
    "print(f\"Test Accuracy : {acc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dffb8f",
   "metadata": {},
   "source": [
    "# Inference Test\n",
    "-Menggunakan algoritma SVM, karena algoritma tersebut mencapai akurasi test yang paling tinggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057351d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n",
      "Input Kalimat Baru : filmnya bagus seru bgt\n",
      "Sentimen kalimat baru adalah POSITIF.\n"
     ]
    }
   ],
   "source": [
    "kalimat_baru = input(\"Masukkan kalimat baru: \")\n",
    "\n",
    "\n",
    "testxax = preprocess_text(kalimat_baru)\n",
    "\n",
    "\n",
    "X_kalimat_baru = tfidf.transform([testxax])\n",
    "\n",
    "\n",
    "prediksi_sentimen = svm.predict(X_kalimat_baru.toarray())\n",
    "\n",
    "\n",
    "if prediksi_sentimen[0] == 'positive':\n",
    "    print(prediksi_sentimen)\n",
    "    print(f\"Input Kalimat Baru : {kalimat_baru}\")\n",
    "    print(\"Sentimen kalimat baru adalah POSITIF.\")\n",
    "else:\n",
    "    print(prediksi_sentimen)\n",
    "    print(f\"Input Kalimat Baru : {kalimat_baru}\")\n",
    "    print(\"Sentimen kalimat baru adalah NEGATIF.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
